{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8879e0b-2ef9-4926-9f3c-392e805f30eb",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbad205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate stopwords flair nltk swifter\n",
    "!pip install gensim\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba37e7-1870-4e93-b79f-0b780fb9eeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf160784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f7fd8-3dbe-4c07-bc2d-bf9c52a9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from flair.data import Sentence\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "import seaborn as sns\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1e3cc-230f-43a8-8567-bee7b846aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "import logging\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c79409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OUR DATASET\n",
    "df = pd.read_csv('./Reviews.csv')\n",
    "# EQUALIZED DATASET\n",
    "sample_df = pd.read_csv('./samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00952f",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f23d6d",
   "metadata": {},
   "source": [
    "### Constant Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y\n",
    "X = np.array(df['Text'])\n",
    "y = np.array(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_y_pred = np.array([5]*len(y_test))\n",
    "accuracy = np.sum(np.equal(y_test, constant_y_pred)) / len(y_test)\n",
    "precision_constant = precision_score(y_test, constant_y_pred, average=None)\n",
    "recall_constant = recall_score(y_test, constant_y_pred, average=None)\n",
    "print(\"accuracy: {}, precision: {}, recall: {}\".format(accuracy, precision_constant, recall_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b96ec",
   "metadata": {},
   "source": [
    "### VADER Analysis Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab85adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the polarity score on the entire dataset\n",
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['Text']\n",
    "    myid = row['Id']\n",
    "    res[myid] = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns={'index': 'Id'})\n",
    "vaders = vaders.merge(df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vaders.loc[:,[\"neg\", \"neu\", \"pos\", \"compound\"]].values)\n",
    "y = np.array(vaders.loc[:,\"Score\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1525949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(tol=0.0001, max_iter=1000, random_state=42,  class_weight=\"balanced\", multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb59db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy, precision, and recall\n",
    "accuracy = np.sum(np.equal(y_test, constant_y_pred)) / len(y_test)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "print(\"accuracy: {}, precision: {}, recall: {}\".format(accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103f7e6",
   "metadata": {},
   "source": [
    "# Word2Vec & NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050501d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Word2VecTrain:\n",
    "    def __init__(self, text_data, sentiment_labels):\n",
    "        self.model = Sequential()\n",
    "        self.max_len = -1\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.embedding_matrix = None\n",
    "        self.text_data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.word2vec_model = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokenizer.fit_on_texts(text_data)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        sequences = self.tokenizer.texts_to_sequences(text_data)\n",
    "        self.max_len = max([len(x) for x in sequences])\n",
    "        self.text_data = pad_sequences(sequences, maxlen=self.max_len)\n",
    "\n",
    "    def build_word2vec_model(self, filePath='./GoogleNews-vectors-negative300.bin.gz'):\n",
    "        self.word2vec_model = KeyedVectors.load_word2vec_format(filePath, binary=True)\n",
    "\n",
    "    def train_test_split(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.text_data, self.sentiment_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Convert sentiment labels to categorical\n",
    "        self.y_train = to_categorical(self.y_train - 1, num_classes=5)  # Assuming you have 5 sentiment labels (1 to 5)\n",
    "        self.y_test = to_categorical(self.y_test - 1, num_classes=5)\n",
    "\n",
    "    def word_embedding_matrix(self, embedding_dim = 300):\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, embedding_dim))\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if word in self.word2vec_model:\n",
    "                self.embedding_matrix[i] = self.word2vec_model[word]\n",
    "\n",
    "    def build_NN(self, embedding_dim=300, layer1=128, layer2=64, dropout=0.3, optimizer='adam', loss='categorical_crossentropy'):\n",
    "        embedding_layer = Embedding(input_dim=self.vocab_size, output_dim=embedding_dim, input_length=self.max_len)\n",
    "        # A three layer neural network\n",
    "        self.model.add(embedding_layer)\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(layer1, activation='relu', input_dim=self.max_len))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        self.model.add(Dense(layer2, activation='relu'))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        self.model.add(Dense(5, activation='softmax'))\n",
    "        self.model.layers[0].set_weights([self.embedding_matrix])\n",
    "        self.model.layers[0].trainable = False\n",
    "        \n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    def train(self, epochs=4, batch_size=32, verbose=1):\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=4, batch_size=32, verbose=1)\n",
    "\n",
    "    def evaluate(self):\n",
    "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test, batch_size=32)\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1) + 1\n",
    "        precision = precision_score(self.y_test.argmax(axis=1) + 1, y_pred, average=None)\n",
    "        recall = recall_score(self.y_test.argmax(axis=1) + 1, y_pred, average=None)\n",
    "        print(\"Test Loss:\", loss)\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(cm)\n",
    "\n",
    "    def get_NN(self):\n",
    "        return self.model\n",
    "\n",
    "    def run(self):\n",
    "        print('tokenizing')\n",
    "        self.tokenize()\n",
    "        print('importing word2vec model')\n",
    "        self.build_word2vec_model()\n",
    "        print('embedding matrix')\n",
    "        self.word_embedding_matrix()\n",
    "        self.train_test_split()\n",
    "        self.build_NN()\n",
    "        print('training NN')\n",
    "        self.train()\n",
    "        print('evaluating NN')\n",
    "        self.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12346ef5-4a67-4b8f-9642-ebb53f47d627",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c8933-1599-4785-96fc-0a5823361fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = df['Text']\n",
    "sentiment_labels = df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20b0b7-35d7-4944-a373-dae838cba2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec = Word2VecTrain(text_data, sentiment_labels)\n",
    "word2vec.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15597260-f4a0-40a9-b18c-6674638c7163",
   "metadata": {},
   "source": [
    "## Shifted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109088a-f87a-49ad-bb2f-dc52dd4a617b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = sample_df['Text']\n",
    "sentiment_labels = sample_df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc32ad1-0363-4956-a1dc-f80ce39d10a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec = Word2VecTrain(text_data, sentiment_labels)\n",
    "word2vec.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c566c6-9a5a-47c0-910c-06de5d6008f7",
   "metadata": {},
   "source": [
    "# Fine Tuned BERT Model Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b797da-e1f8-4319-aa57-25824d8c1b76",
   "metadata": {},
   "source": [
    "### Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f619c1c-0d36-4133-ae84-07c80ff48a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this dataset if you want to run on the entire data\n",
    "amazon_reviews_df = pd.read_csv('./Reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973c10b-454a-4f81-ba06-591e8ac460af",
   "metadata": {},
   "source": [
    "### Equalized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f859078-9941-4f24-80d7-ce7f1724f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dataset if you want to run on the equalized data\n",
    "amazon_reviews_df = pd.read_csv('./samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5477e",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959511b-2ec3-4d3b-85fc-3021c6a776c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_df.dropna(axis=0, subset = ['Score', 'Summary'], inplace=True)\n",
    "\n",
    "#Quick view on data distribution\n",
    "ax = amazon_reviews_df['Score'].value_counts().sort_index().plot(kind='bar',\n",
    "          title='Count of Reviews by Stars',\n",
    "          figsize=(10, 5))\n",
    "ax.set_xlabel('Review Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c391dd6-ce8c-4dd0-972f-84e2da1eacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points in each star category\n",
    "np.unique(amazon_reviews_df.Score,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678bd03-3074-4491-91c6-7458a2aa6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29d66a-1a37-47c7-9bae-a30687f29f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "amazon_reviews_df['Text'] = amazon_reviews_df['Text'].apply(lambda row : re.sub('<.*?>', '', row))\n",
    "\n",
    "# TFBertForSequenceClassification requires labels in the range [0,1,...]\n",
    "amazon_reviews_df[\"Score\"] = amazon_reviews_df[\"Score\"].subtract(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe80f0f-3bf8-48c0-8260-e93bfafba815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table: Number of datapoints per category\n",
    "temp = amazon_reviews_df.groupby('Score').count()['Text'].reset_index().sort_values(by='Text',ascending=False)\n",
    "temp.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489a912-5689-41c7-a5ef-328057984ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: Distribution of the number of words\n",
    "word_count = amazon_reviews_df\n",
    "word_count['num_words'] = word_count['Text'].apply(lambda x : len(str(x).split()))\n",
    "sns.histplot(data=word_count, x = 'num_words', bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d8568-dfc1-4a29-8216-88fd90e9b7cc",
   "metadata": {},
   "source": [
    "### WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5446f08-0034-454f-9d73-e719e7b86e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide reviews into positive and negative based on the scores\n",
    "amazon_reviews_df.loc[amazon_reviews_df['Score'] <= 3, 'Score'] = 0\n",
    "amazon_reviews_df.loc[amazon_reviews_df['Score'] == 5, 'Score'] = 1\n",
    "amazon_reviews_df.drop(amazon_reviews_df[amazon_reviews_df['Score']==4].index, inplace=True)\n",
    "\n",
    "positive = amazon_reviews_df[amazon_reviews_df['Score'] == 1]\n",
    "negative = amazon_reviews_df[amazon_reviews_df['Score'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c689a2-fa9c-4232-bd99-d929745535de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or wordcloud by score\n",
    "\n",
    "one = amazon_reviews_df[amazon_reviews_df['Score'] == 0]\n",
    "two = amazon_reviews_df[amazon_reviews_df['Score'] == 1]\n",
    "three = amazon_reviews_df[amazon_reviews_df['Score'] == 2]\n",
    "four = amazon_reviews_df[amazon_reviews_df['Score'] == 3]\n",
    "five = amazon_reviews_df[amazon_reviews_df['Score'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d68fd-d1ac-45bb-b513-9b45bcd217b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive reviews\n",
    "txt = ' '.join(rev for rev in positive['Text'])\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "            background_color = 'black',\n",
    "            max_font_size = 100,\n",
    "            max_words = 100,\n",
    "            width = 1000,\n",
    "            height = 600\n",
    "            ).generate(txt)\n",
    "\n",
    "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b1b64-6b45-4e2c-8a99-5357eb37652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative reviews\n",
    "txt = ' '.join(rev for rev in negative['Text'])\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "            background_color = 'black',\n",
    "            max_font_size = 100,\n",
    "            max_words = 100,\n",
    "            width = 1000,\n",
    "            height = 600\n",
    "            ).generate(txt)\n",
    "\n",
    "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90e4f9-d0fc-42f3-985d-438f8345ca82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67ab5550-aa86-407f-9a8c-0fff4870e247",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120f15e-2835-471c-87f8-63998df46aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit input string length\n",
    "amazon_reviews_df['Text'] = amazon_reviews_df['Text'].str[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8a464-4a95-4d58-86cc-c0e1c8a5603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = amazon_reviews_df[\"Text\"].values.tolist()\n",
    "scores = amazon_reviews_df[\"Score\"].tolist()\n",
    "\n",
    "# Split the dataset into train, validation and holdout sets (60-20-20)\n",
    "training_sentences, test_sentences, training_labels, test_labels = train_test_split(reviews, scores, test_size=.4)\n",
    "validation_sentences, holdout_sentences, validation_labels, holdout_labels = train_test_split(test_sentences, test_labels, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1030f-a26e-4892-99ca-d9f4c02f16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Our Input Data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48272e13-cd26-402b-a10c-974a527c06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(training_sentences,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "validation_encodings = tokenizer(validation_sentences,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "holdout_encodings = tokenizer(holdout_sentences,\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ef016-03f3-4e3b-9d66-60b9be8cc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input encodings and labels into a Dataset object\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(train_encodings),\n",
    "                            training_labels\n",
    "                            ));\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(validation_encodings),\n",
    "                            validation_labels\n",
    "                            ));\n",
    "\n",
    "holdout_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(holdout_encodings),\n",
    "                            holdout_labels\n",
    "                            ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff99567-09c0-4442-a29d-446b344dabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our pre-trained BERT model\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c85c6-7ba1-476a-90f9-d5bdf5505125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and fine tune our pre-trained BERT model\n",
    "\n",
    "history = model.fit(train_dataset.shuffle(30).batch(8),\n",
    "          epochs=1,\n",
    "          batch_size=8,\n",
    "          validation_data=validation_dataset.shuffle(30).batch(8), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bb17f-a791-4a65-9433-73b8b791c0c0",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f9f8d-bca5-4745-b1da-0ab0e3653a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and then evaluate it on holdout set\n",
    "\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(\"./output_model\")\n",
    "result = model.evaluate(holdout_dataset.batch(8))\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1f51a-3a5f-4a89-adc8-9a8eaa9cdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment for holdout set\n",
    "\n",
    "tf_output = loaded_model.predict(holdout_dataset.batch(8))\n",
    "pred_label = tf.argmax(tf.nn.softmax(tf_output[\"logits\"], axis=1).numpy(), 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902189af-1903-43e5-a5b5-781256a1515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeModelCompetence(labels, true_labels, pred_labels):\n",
    "    # Confusion Matrix plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    cm = confusion_matrix(y_true=true_labels, y_pred=pred_labels, labels=range(len(labels)), normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax)\n",
    "    \n",
    "    # Precision and Recall Scores\n",
    "    precision = precision_score(true_labels, pred_labels, average=None)\n",
    "    recall = recall_score(true_labels, pred_labels, average=None)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb135ce-4f70-46c2-8786-df3b8bbbeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix and get precision and recall scores\n",
    "# Note that due to TFBertForSequenceClassification, labels [1,2,3,4,5] are shown as [0,1,2,3,4]\n",
    "labels = [0,1,2,3,4]\n",
    "precision, recall = VisualizeModelCompetence(labels, holdout_labels, pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cc5c2",
   "metadata": {},
   "source": [
    "# Fine Tuned GPT2Model Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b881c0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ad79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = labels\n",
    "        self.texts = [self.tokenizer(text, padding='max_length', max_length=self.max_length, truncation=True, return_tensors=\"pt\") for text in texts]\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], np.array(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98dd39e",
   "metadata": {},
   "source": [
    "### Define Classifier nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15667b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Classifier(nn.Module):\n",
    "    def __init__(self, max_len:int):\n",
    "        super(GPT2Classifier,self).__init__()\n",
    "        \n",
    "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        self.out = nn.Linear(768 * max_len, 5) #gpt2's final layer has (768*max_len) neurons\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        out, _ = self.gpt2(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = out.shape[0]\n",
    "        out = self.out(out.view(batch_size,-1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e8cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d22f52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GPT2Train(model, train_dataloader, train_len, val_dataloader, val_len, epochs, lr):\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(input_id, mask) # output is the attention scores, not the actual outputs\n",
    "\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1)==train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = total_loss_train / train_len\n",
    "        train_acc = total_acc_train / train_len\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1)==val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "            val_loss = total_loss_val / val_len\n",
    "            val_acc = total_acc_val / val_len\n",
    "\n",
    "            print(f\"Epochs: {epoch_num + 1}\\n\"\n",
    "                  f\"Train Loss: {train_loss} | Train Accuracy: {train_acc} | Val Loss: {val_loss} | Val Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf1f7e",
   "metadata": {},
   "source": [
    "### Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT2Evaluate(model, test_dataloader, test_len):\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            true_labels += test_label.cpu().numpy().flatten().tolist()\n",
    "            pred_labels += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "\n",
    "    test_acc = total_acc_test / test_len\n",
    "\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "    return true_labels, pred_labels, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f089f4",
   "metadata": {},
   "source": [
    "### Evaluation Metrics and Visualization: confusion matrix, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f70a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeModelCompetence(labels, true_labels, pred_labels):\n",
    "    # Confusion Matrix plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    cm = confusion_matrix(y_true=true_labels, y_pred=pred_labels, labels=range(len(labels)), normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax)\n",
    "    \n",
    "    # Precision and Recall Scores\n",
    "    precision = precision_score(true_labels, pred_labels, average=None)\n",
    "    recall = recall_score(true_labels, pred_labels, average=None)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cde5e",
   "metadata": {},
   "source": [
    "### Run with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f75376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load dataset\n",
    "data = np.array(df['Text'])  # List of input texts\n",
    "labels = np.array(df['Score'] - 1)  # List of labels (0-4)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=seed)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa07cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer, GPT2Tokenizer requires that padding is on the left side\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f840bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and model\n",
    "max_len = 32\n",
    "epochs = 3\n",
    "lr = 1e-5\n",
    "model = GPT2Classifier(max_len=max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and dataloader\n",
    "train_dataset = SentimentDataset(train_data, train_labels, max_length=max_len, tokenizer=tokenizer) # takes a while...\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "train_len = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation dataset and dataloader\n",
    "val_dataset = SentimentDataset(val_data, val_labels, max_length=max_len, tokenizer=tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "val_len = len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3207fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "GPT2Train(model, train_dataloader, train_len, val_dataloader, val_len, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db52a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and dataloader\n",
    "test_dataset = SentimentDataset(test_data, test_labels, max_length=max_len, tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "test_len = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "true_labels, pred_labels, test_acc = GPT2Evaluate(model, test_dataloader, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df959523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix and get precision and recall scores\n",
    "labels = [1,2,3,4,5]\n",
    "precision, recall = VisualizeModelCompetence(labels, true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Test Accuracy: {text_acc}\\n'\n",
    "    f'Test Precision: {precision}\\n'\n",
    "    f'Test Recall: {recall}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65ac7f",
   "metadata": {},
   "source": [
    "### Testing Dataset Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "sample_data = np.array(sample_df['Text'])  # List of input texts\n",
    "sample_labels = np.array(sample_df['Score'] - 1)  # List of labels (0-4)\n",
    "s_train_data, s_test_data, s_train_labels, s_test_labels = train_test_split(sample_data, sample_labels, test_size=0.2, random_state=seed)\n",
    "s_train_data, s_val_data, s_train_labels, s_val_labels = train_test_split(s_train_data, s_train_labels, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6975ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters and model\n",
    "epochs = 1\n",
    "lr = 1e-5\n",
    "s_model = GPT2Classifier(max_len=max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train dataset and dataloader\n",
    "s_train_dataset = SentimentDataset(s_train_data, s_train_labels, max_length=max_len, tokenizer=tokenizer)\n",
    "s_train_dataloader = DataLoader(s_train_dataset, batch_size=64, shuffle=True)\n",
    "s_train_len = len(s_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation dataset and dataloader\n",
    "s_val_dataset = SentimentDataset(s_val_data, s_val_labels, max_length=max_len, tokenizer=tokenizer)\n",
    "s_val_dataloader = DataLoader(s_val_dataset, batch_size=64, shuffle=True)\n",
    "s_val_len = len(s_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "GPT2Train(s_model, s_train_dataloader, s_train_len, s_val_dataloader, s_val_len, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test dataset and dataloader\n",
    "s_test_dataset = SentimentDataset(s_test_data, s_test_train_labels, max_length=max_len, tokenizer=tokenizer)\n",
    "s_test_dataloader = DataLoader(s_test_train_dataset, batch_size=64, shuffle=True)\n",
    "s_test_len = len(s_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "true_labels, pred_labels, test_acc = GPT2Evaluate(s_model, s_test_dataloader, s_test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix and get precision and recall scores\n",
    "labels = [1,2,3,4,5]\n",
    "precision, recall = VisualizeModelCompetence(labels, true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Test Accuracy: {text_acc}\\n'\n",
    "    f'Test Precision: {precision}\\n'\n",
    "    f'Test Recall: {recall}'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
